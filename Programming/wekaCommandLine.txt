weka.classifiers
Classifiers are at the core of WEKA. There are a lot of common options for classifiers, most of which are related to evaluation purposes. We will focus on the most important ones. All others including classifier-specific parameters can be found via -h, as usual.

-tspecifies the training file (ARFF format)-Tspecifies the test file in (ARFF format). If this parameter is missing, a crossvalidation will be performed (default: 10-fold cv)-xThis parameter determines the number of folds for the cross-validation. A cv will only be performed if -T is missing.-cAs we already know from the weka.filters section, this parameter sets the class variable with a one-based index.-dThe model after training can be saved via this parameter. Each classifier has a different binary format for the model, so it can only be read back by the exact same classifier on a compatible dataset. Only the model on the training set is saved, not the multiple models generated via cross-validation.-lLoads a previously saved model, usually for testing on new, previously unseen data. In that case, a compatible test file should be specified, i.e. the same attributes in the same order.-p <attrib_range>If a test file is specified, this parameter shows you the predictions and one attribute (0 for none) for all test instances. If no test file is specified, this outputs nothing. In that case, you will have to use callClassifier from Appendix A.-iA more detailed performance description via precision, recall, true- and false positive rate is additionally output with this parameter. All these values can also be computed from the confusion matrix.-oThis parameter switches the human-readable output of the model description off. In case of support vector machines or NaiveBayes, this makes some sense unless you want to parse and visualize a lot of information.We now give a short list of selected classifiers in WEKA. Other classifiers below weka.classifiers in package overview may also be used. This is more easy to see in the Explorer GUI.

* trees.J48 A clone of the C4.5 decision tree learner
* bayes.NaiveBayes A Naive Bayesian learner. -K switches on kernel density estimation for numerical attributes which often improves performance.
* meta.ClassificationViaRegression -W functions.LinearRegression Multi-response linear regression.
* functions.Logistic Logistic Regression.
* functions.SMO Support Vector Machine (linear, polynomial and RBF kernel) with Sequential Minimal Optimization Algorithm due to [Platt, 1998]. Defaults to SVM with linear kernel, -E 5 -C 10 gives an SVM with polynomial kernel of degree 5 and lambda=10.
* lazy.KStar Instance-Based learner. -E sets the blend entropy automatically, which is usually preferable.
* lazy.IBk Instance-Based learner with fixed neighborhood. -K sets the number of neighbors to use. IB1 is equivalent to IBk -K 1
* rules.JRip A clone of the RIPPER rule learner.
* java callClassifier weka.classifiers.trees.J48 -t iris.arff
* java.lang.Object
*   weka.classifiers.Classifier
* All Implemented Interfaces:
* java.lang.Cloneable, OptionHandler, java.io.Serializable
* Direct Known Subclasses:
* AdditiveRegression, ADTree, AODE, AttributeSelectedClassifier, BayesNet, ComplementNaiveBayes, ConjunctiveRule, CostSensitiveClassifier, DecisionStump, DecisionTable, Decorate,FilteredClassifier, FLR, HND, HyperPipes, IB1, IBk, Id3, J48, JRip, KStar, LBR, LeastMedSq, LinearRegression, LMT, Logistic, LogisticBase, M5Base, MultiClassClassifier, MultilayerPerceptron,MultipleClassifiersCombiner, NaiveBayes, NaiveBayesMultinomial, NaiveBayesSimple, ND, NNge, OneR, OrdinalClassClassifier, PaceRegression, PART, PreConstructedLinearModel, Prism,RacedIncrementalLogitBoost, RandomForest, RandomizableClassifier, RandomTree, RBFNetwork, REPTree, Ridor, RuleNode, SimpleLinearRegression, SimpleLogistic, SingleClassifierEnhancer,SMO, SMOreg, ThresholdSelector, TreeBasedMultiClassClassifier, UserClassifier, VFI, VotedPerceptron, Winnow, ZeroR

