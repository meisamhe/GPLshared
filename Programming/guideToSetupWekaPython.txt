First download and install the installer and pip from:
http://www.lfd.uci.edu/~gohlke/pythonlibs/#setuptools
http://www.lfd.uci.edu/~gohlke/pythonlibs/#setuptools
http://www.lfd.uci.edu/~gohlke/pythonlibs/#pip

cmd
C:\Python27\Scripts\pip.exe install blinkstick
python -m pip install pyparsing
python -m pip install numpy
python -m pip install matplotlib
python -m pip install Pillow
python -m pip install PIL
python -m pip install six
python -m easy_install wxPython
python -m pip install imaging
python -m easy_install imaging
python -m pip install javabridge

Manual:
=====================
pygraphviz
Scipy
opencv 
tesseract 
Assimulo 
ffmpeg
Ulipad

set pythonn path:
set PYTHONPATH=%PYTHONPATH%;C:\Python27



Found out what was wrong. I never installed the setuptools for python, so it was missing some vital files, like the egg ones.

If you find yourself having my issue above, download this file and then in powershell or command prompt, navigate to ez_setup’s directory and execute the command python ez_setup.py and this will run the file for you.

If you still need to install pip at this point, just type in easy_install pip and hit enter. easy_install was part of the setuptools, and therefore wouldn't work for installing pip.

Then, pip will successfully install django with the command pip install django

Hope I saved someone the headache I gave myself!

~Zorpix

Path of WEKA is: https://github.com/chrisspen/weka

To run in Windows:

Add weka.jar, libsvm.jar, wlsvm.jar to C:\usr\share\java directory (libsvm.jar and wlsvm.jar can be found at http://www.cs.iastate.edu/~yasser/wlsvm/)
run pip install -U https://github.com/chrisspen/weka/tarball/master
When you try to run classifiers you will get a classpath error. To solve this error edit installed file <Python install dir>\Lib\site-packages\weka\classifiers.py
Line 33: Change for _cp in CP.split(':'): to for _cp in CP.split(os.pathsep):
Line 286: Change close_fds=True to close_fds=sys.platform != "win32"

1. Install the setup tool of python by simply "python ez_setup.py" (the file is in the same folder)

python -m pip install -U pip

python -m pip install virtualenv

python -m pip install numpy

python -m install javabridge

Download and install java jdk from oracle website

# Make sure numpy is installed
python setup.py install

python -m pip install pil

python -m pip install Pillow

python -m pip install mathplotlib 

python -m pip install pygraphviz

python -m pip install git://github.com/pygraphviz/pygraphviz.git#egg=pygraphviz

python -m pip install javabridge

Installing pip:
c:\Python27\python.exe c:\distribute_setup.py
c:\Python27\Scripts\easy_install.exe pip

# easy install command is in scripts of Python so I should navigate to the folder
easy_install setuptools_git

 easy_install GitPython

Navigate to distribution of WekaPython and run:

python setup.py install

cd "C:\Users\MeisamHe\Desktop\BackupToRestoreComputerApril15\MachineLearning\HW\HW3\weka-0.1.3.tar\dist\weka-0.1.3\weka-0.1.3"

OpenWatcom as a good compiler of C is needed to install the wrapper of Weka in Python

The instruction tells to take it from: aka.ms/vcpython27

As a result I took it and installed it

Git hub of python (in bit bucket bash run...):
$ git clone git://github.com/gitpython-developers/GitPython.git git-python
$ cd git-python
$git submodule update --init --recursive

start without problem:
=================
first move all the jar files to the jar folder in the Python27:
----------------------------------------------------
import weka.core.jvm as jvm
jvm.start(class_path=['C:/Python27/jars/python-weka-wrapper.jar','C:/Python27/jars/weka.jar'])

from weka.core.classes import Random
from weka.core.converters import Loader
from weka.filters import Filter
from weka.classifiers import Classifier, Evaluation

loader = Loader()
data = loader.load_file("C:/Python27/jars/acq.arff")

data.set_class_index(data.num_attributes()-1)

#=================================================================
Artifical data can be generated using one of Weka’s data generators, e.g., the Agrawal classification generator:

>>> from weka.datagenerators import DataGenerator
>>> generator = DataGenerator(classname="weka.datagenerators.classifiers.classification.Agrawal", options=["-B", "-P", "0.05"])
>>> DataGenerator.make_data(generator, ["-o", "/some/where/outputfile.arff"])

Or using the low-level API (outputting data to stdout):

>>> generator = DataGenerator(classname="weka.datagenerators.classifiers.classification.Agrawal", options=["-n", "10", "-r", "agrawal"])
>>> generator.set_dataset_format(generator.define_data_format())
>>> print(generator.get_dataset_format())
>>> if generator.get_single_mode_flag():
>>>     for i in xrange(generator.get_num_examples_act()):
>>>         print(generator.generate_example())
>>> else:
>>>     print(generator.generate_examples())

#===============================================================
Loaders and Savers¶
#================================================================
You can load and save datasets of various data formats using the Loader and Saver classes.

The following example loads an ARFF file and saves it as CSV:

>>> from weka.core.converters import Loader, Saver
>>> loader = Loader(classname="weka.core.converters.ArffLoader")
>>> data = loader.load_file("/some/where/iris.arff")
>>> print(data)
>>> saver = Saver(classname="weka.core.converters.CSVSaver")
>>> saver.save_file(data, "/some/where/iris.csv")

#===================================================================
Filters
#======================================================================
The Filter class from the weka.filters module allows you to filter datasets, e.g., removing the last attribute using the Remove filter:

>>> from weka.filters import Filter
>>> data = ...                       # previously loaded data
>>> remove = Filter(classname="weka.filters.unsupervised.attribute.Remove", options=["-R", "last"])
>>> remove.set_inputformat(data)     # let the filter know about the type of data to filter
>>> filtered = remove.filter(data)   # filter the data
>>> print(filtered)                  # output the filtered data

#=========================================================================
Classifiers
#=========================================================================
Here is an example on how to cross-validate a J48 classifier (with confidence factor 0.3) on a dataset and output the summary and some specific statistics:

>>> from weka.classifiers import Classifier, Evaluation
>>> from weka.core.classes import Random
>>> data = ...                                        # previously loaded data
>>> data.set_class_index(data.num_attributes() - 1)   # set class attribute
>>> classifier = Classifier(classname="weka.classifiers.trees.J48", options=["-C", "0.3"])
>>> evaluation = Evaluation(data)                     # initialize with priors
>>> evaluation.crossvalidate_model(classifier, iris_data, 10, Random(42))  # 10-fold CV
>>> print(evaluation.to_summary())
>>> print("pctCorrect: " + str(evaluation.percent_correct()))
>>> print("incorrect: " + str(evaluation.incorrect()))

#========================================================
Clusters
#===========================================================
In the following an example on how to build a SimpleKMeans (with 3 clusters) using a previously loaded dataset without a class attribute:


>>> from weka.clusterers import Clusterer
>>> data = ... # previously loaded dataset
>>> clusterer = Clusterer(classname="weka.clusterers.SimpleKMeans", options=["-N", "3"])
>>> clusterer.build_clusterer(data)
>>> print(clusterer)

#===========================================================
Attribute Selection
#===========================================================
>>> from weka.attribute_selection import ASSearch, ASEvaluation, AttributeSelection
>>> data = ...   # previously loaded dataset
>>> search = ASSearch(classname="weka.attributeSelection.BestFirst", options=["-D", "1", "-N", "5"])
>>> evaluator = ASEvaluation(classname="weka.attributeSelection.CfsSubsetEval", options=["-P", "1", "-E", "1"])
>>> attsel = AttributeSelection()
>>> attsel.set_search(search)
>>> attsel.set_evaluator(evaluator)
>>> attsel.select_attributes(data)
>>> print("# attributes: " + str(attsel.get_number_attributes_selected()))
>>> print("attributes: " + str(attsel.get_selected_attributes()))
>>> print("result string:\n" + attsel.to_results_string())

#==========================================================
Associators
#=========================================================
Associators, like Apriori, can be built and output like this:
>>> from weka.associations import Associator
>>> data = ...   # previously loaded dataset
>>> associator = Associator(classname="weka.associations.Apriori", options=["-N", "9", "-I"])
>>> associator.build_associations(data)
>>> print(associator)

#==============================================================
Serialization
#==============================================================
You can easily serialize and de-serialize as well.

Here we just save a trained classifier to a file, load it again from disk and output the model:

>>> import weka.core.serialization as serialization
>>> from weka.classifiers import Classifier
>>> classifier = ...  # previously built classifier
>>> serialization.write("/some/where/out.model", classifier)
>>> ...
>>> classifier2 = Classifier(jobject=serialization.read("/some/where/out.model"))
>>> print(classifier2)

Weka usually saves the header of the dataset that was used for training as well (e.g., in order to determine whether test data is commpatible). This is done as follows:

>>> import weka.core.serialization as serialization
>>> from weka.classifiers import Classifier
>>> from weka.core.dataset import Instances
>>> classifier = ...  # previously built Classifier
>>> data = ... # previously loaded/generated Instances
>>> serialization.write_all("/some/where/out.model", [classifier, Instances.template_instances(data)])
>>> ...
>>> objects = serialization.read_all("/some/where/out.model")
>>> classifier2 = Classifier(jobject=objects[0])
>>> data2 = Instances(jobject=objects[1])
>>> print(classifier2)
>>> print(data2)


#============================================================
Experiments
#============================================================
Experiments, like they are run in Weka’s Experimenter, can be configured and executed as well.

Here is an example for performing a cross-validated classification experiment:

>>> from weka.experiments import SimpleCrossValidationExperiment, SimpleRandomSplitExperiment, Tester, ResultMatrix
>>> from weka.classifiers import Classifier
>>> import weka.core.converters as converters
>>> # configure experiment
>>> datasets = ["iris.arff", "anneal.arff"]
>>> classifiers = [Classifier(classname="weka.classifiers.rules.ZeroR"), Classifier(classname="weka.classifiers.trees.J48")]
>>> outfile = "results-cv.arff"   # store results for later analysis
>>> exp = SimpleCrossValidationExperiment(
>>>     classification=True,
>>>     runs=10,
>>>     folds=10,
>>>     datasets=datasets,
>>>     classifiers=classifiers,
>>>     result=outfile)
>>> exp.setup()
>>> exp.run()
>>> # evaluate previous run
>>> loader = converters.loader_for_file(outfile)
>>> data   = loader.load_file(outfile)
>>> matrix = ResultMatrix(classname="weka.experiment.ResultMatrixPlainText")
>>> tester = Tester(classname="weka.experiment.PairedCorrectedTTester")
>>> tester.set_resultmatrix(matrix)
>>> comparison_col = data.get_attribute_by_name("Percent_correct").get_index()
>>> tester.set_instances(data)
>>> print(tester.header(comparison_col))
>>> print(tester.multi_resultset_full(0, comparison_col))

And a setup for performing regression experiments on random splits on the datasets:

>>> from weka.experiments import SimpleCrossValidationExperiment, SimpleRandomSplitExperiment, Tester, ResultMatrix
>>> from weka.classifiers import Classifier
>>> import weka.core.converters as converters
>>> # configure experiment
>>> datasets = ["bolts.arff", "bodyfat.arff"]
>>> classifiers = [Classifier(classname="weka.classifiers.rules.ZeroR"), Classifier(classname="weka.classifiers.functions.LinearRegression")]
>>> outfile = "results-rs.arff"   # store results for later analysis
>>> exp = SimpleRandomSplitExperiment(
>>>     classification=False,
>>>     runs=10,
>>>     percentage=66.6,
>>>     preserve_order=False,
>>>     datasets=datasets,
>>>     classifiers=classifiers,
>>>     result=outfile)
>>> exp.setup()
>>> exp.run()
>>> # evaluate previous run
>>> loader = converters.loader_for_file(outfile)
>>> data   = loader.load_file(outfile)
>>> matrix = ResultMatrix(classname="weka.experiment.ResultMatrixPlainText")
>>> tester = Tester(classname="weka.experiment.PairedCorrectedTTester")
>>> tester.set_resultmatrix(matrix)
>>> comparison_col = data.get_attribute_by_name("Correlation_coefficient").get_index()
>>> tester.set_instances(data)
>>> print(tester.header(comparison_col))
>>> print(tester.multi_resultset_full(0, comparison_col))

#===================================================
Packages
#=====================================================
Packages can be listed, installed and uninstalled using the weka.core.packages module:

# list all packages (name and URL)
import weka.core.packages as packages
items = packages.get_all_packages()
for item in items:
    print item.get_name(), item.get_url()

# install CLOPE package
packages.install_package("CLOPE")
items = packages.get_installed_packages()
for item in items:
    print item.get_name(), item.get_url()

# uninstall CLOPE package
packages.uninstall_package("CLOPE")
items = packages.get_installed_packages()
for item in items:
    print item.get_name(), item.get_url()

#=================================================
Naive Bayes
#====================================================
loader = Loader(classname="weka.core.converters.ArffLoader")
iris_inc = loader.load_file(data_dir + "iris.arff", incremental=True)
iris_inc.set_class_index(iris_inc.num_attributes() - 1)

print(iris_inc)

cls = Classifier(classname="weka.classifiers.bayes.NaiveBayesUpdateable")
cls.build_classifier(iris_inc)
for inst in loader:
    cls.update_classifier(inst)

print(cls)

Valid options are:

 -K
  Use kernel density estimator rather than normal
  distribution for numeric attributes
 -D
  Use supervised discretization to process numeric attributes
 
 -O
  Display model in old format (good when there are many classes)

#==================================================
Logistic Regression
#=====================================================
 
java.lang.Object
weka.classifiers.AbstractClassifier
weka.classifiers.bayes.NaiveBayes
weka.classifiers.bayes.NaiveBayesUpdateable
All Implemented Interfaces:
java.io.Serializable, java.lang.Cloneable, Classifier, UpdateableClassifier, Aggregateable<NaiveBayes>, CapabilitiesHandler, OptionHandler, RevisionHandler, TechnicalInformationHandler, WeightedInstancesHandler

public class NaiveBayesUpdateable
extends NaiveBayes
implements UpdateableClassifier
Class for a Naive Bayes classifier using estimator classes. This is the updateable version of NaiveBayes.
This classifier will use a default precision of 0.1 for numeric attributes when buildClassifier is called with zero training instances.

Valid options are:

 -K
  Use kernel density estimator rather than normal
  distribution for numeric attributes
 -D
  Use supervised discretization to process numeric attributes
 
 -O
  Display model in old format (good when there are many classes)

#============================================================
Multilayered Perception
#===============================================================
public class MultilayerPerceptron
extends AbstractClassifier
implements OptionHandler, WeightedInstancesHandler, Randomizable

Valid options are:

 -L <learning rate>
  Learning Rate for the backpropagation algorithm.
  (Value should be between 0 - 1, Default = 0.3).
 
 -M <momentum>
  Momentum Rate for the backpropagation algorithm.
  (Value should be between 0 - 1, Default = 0.2).
 
 -N <number of epochs>
  Number of epochs to train through.
  (Default = 500).
 
 -V <percentage size of validation set>
  Percentage size of validation set to use to terminate
  training (if this is non zero it can pre-empt num of epochs.
  (Value should be between 0 - 100, Default = 0).
 
 -S <seed>
  The value used to seed the random number generator
  (Value should be >= 0 and and a long, Default = 0).
 
 -E <threshold for number of consequetive errors>
  The consequetive number of errors allowed for validation
  testing before the netwrok terminates.
  (Value should be > 0, Default = 20).
 
 -G
  GUI will be opened.
  (Use this to bring up a GUI).
 
 -A
  Autocreation of the network connections will NOT be done.
  (This will be ignored if -G is NOT set)
 
 -B
  A NominalToBinary filter will NOT automatically be used.
  (Set this to not use a NominalToBinary filter).
 
 -H <comma seperated numbers for nodes on each layer>
  The hidden layers to be created for the network.
  (Value should be a list of comma separated Natural 
  numbers or the letters 'a' = (attribs + classes) / 2, 
  'i' = attribs, 'o' = classes, 't' = attribs .+ classes)
  for wildcard values, Default = a).
 
 -C
  Normalizing a numeric class will NOT be done.
  (Set this to not normalize the class if it's numeric).
 
 -I
  Normalizing the attributes will NOT be done.
  (Set this to not normalize the attributes).
 
 -R
  Reseting the network will NOT be allowed.
  (Set this to not allow the network to reset).
 
 -D
  Learning rate decay will occur.
  (Set this to cause the learning rate to decay).
 



Some arff data to test:
https://www.mat.unical.it/OlexSuite/Datasets/SampleDataSets-download.htm
